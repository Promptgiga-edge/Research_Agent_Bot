# Research Agent - Complete Workflow & Architecture Diagram

## ğŸ—ï¸ **Project Architecture Overview**

```
Research_Agent/
â”œâ”€â”€ ğŸ“± app.py                    # Streamlit Web Application (Entry Point)
â”œâ”€â”€ âš™ï¸ config.py               # Configuration Management
â”œâ”€â”€ ğŸ¤– research_agent.py       # Main Research Agent (LangGraph Workflow)
â”œâ”€â”€ ğŸ“„ document_processor.py   # PDF/HTML Document Processing
â”œâ”€â”€ ğŸ—ƒï¸ vector_store.py         # ChromaDB Vector Database Management
â”œâ”€â”€ ğŸ” scholar_api.py          # Google Scholar API Integration
â”œâ”€â”€ ğŸ§  gemini_client.py        # Google Gemini AI Client
â”œâ”€â”€ ğŸ”§ geminiClientWrapper.py  # Alternative Gemini Client Wrapper
â”œâ”€â”€ ğŸ”§ health_check.py         # Logging Setup
â”œâ”€â”€ ğŸ”§ logging_config.py.py    # Logging Configuration (Duplicate)
â”œâ”€â”€ ğŸ§ª test_gemini.py         # Gemini API Tests
â”œâ”€â”€ ğŸ§ª test_full_integration.py # Full Integration Tests
â””â”€â”€ ğŸ“¦ setup.py               # Package Setup
```

---

## ğŸ“‹ **Complete Workflow - File by File**

### 1. **ğŸ“± app.py** - Streamlit Web Application
**Main Entry Point** | **User Interface**

#### Classes:
- `StreamlitApp` - Main application class

#### Key Methods:
```python
StreamlitApp.__init__()                 # Initialize app
StreamlitApp.initialize_agent()         # Setup research agent
StreamlitApp.run()                      # Main app runner
StreamlitApp.render_sidebar()           # Sidebar with stats/controls
StreamlitApp.render_main_content()      # Main query interface
StreamlitApp.process_query(query: str)  # Process user queries
StreamlitApp.display_chat_history()     # Show previous Q&A
StreamlitApp.display_user_message()     # Format user messages
StreamlitApp.display_assistant_message() # Format AI responses
StreamlitApp.display_error_message()    # Error message formatting
StreamlitApp.create_visualizations()    # Generate charts & graphs
StreamlitApp._format_authors()          # Format author names
StreamlitApp._format_authors_list()     # Format author lists
main()                                  # Application entry point
```

#### Dependencies & Calls:
```
â†’ ResearchAgent (from research_agent.py)
â†’ config (from config.py)
â†’ Uses Streamlit, Plotly, Pandas
â†’ Calls: agent.research(), agent.get_vector_store_stats(), agent.clear_vector_store()
```

---

### 2. **âš™ï¸ config.py** - Configuration Management
**Settings & Environment Variables**

#### Classes:
- `Config` - Configuration dataclass

#### Key Attributes:
```python
Config.GEMINI_API_KEY          # Google Gemini API key
Config.GEMINI_MODEL           # Model name (gemini-2.5-flash)
Config.SERPAPI_KEY            # SerpAPI key for Google Scholar
Config.MAX_RESULTS            # Maximum search results
Config.MAX_TOKENS             # Token limit for AI responses
Config.TEMPERATURE            # AI creativity setting
Config.VECTOR_DB_PATH         # ChromaDB storage path
Config.EMBEDDING_MODEL        # Sentence transformer model
Config.CACHE_DIR              # Document cache directory
Config.CACHE_EXPIRY_HOURS     # Cache expiration time
Config.APP_TITLE              # Application title
Config.APP_DESCRIPTION        # App description
```

#### Key Methods:
```python
Config.validate() -> bool      # Validate required settings
```

#### Dependencies:
```
â†’ os, dataclasses, dotenv
â†’ Loads from .env file
```

---

### 3. **ğŸ¤– research_agent.py** - Main Research Agent
**Core AI Workflow Engine** | **LangGraph Implementation**

#### Classes:
- `ResearchState` - TypedDict for workflow state
- `QueryRefinementTool` - Query optimization tool  
- `PaperAnalysisTool` - Paper content analysis tool
- `ResearchAgent` - Main agent orchestrator

#### Key Methods:
```python
# ResearchAgent
ResearchAgent.__init__()                    # Initialize all components
ResearchAgent._build_graph() -> StateGraph  # Build LangGraph workflow
ResearchAgent._refine_query()               # Step 1: Optimize search query
ResearchAgent._search_papers()              # Step 2: Find relevant papers
ResearchAgent._process_documents()          # Step 3: Extract paper content
ResearchAgent._extract_context()            # Step 4: Get relevant chunks
ResearchAgent._generate_answer()            # Step 5: Create AI response
ResearchAgent._error_handler()              # Handle workflow errors
ResearchAgent.research(query: str) -> Dict  # Main research function
ResearchAgent.get_vector_store_stats()      # Get database stats
ResearchAgent.clear_vector_store()          # Clear stored documents

# QueryRefinementTool
QueryRefinementTool.__init__(llm)          # Initialize with LLM
QueryRefinementTool.refine_query() -> str  # Optimize search query

# PaperAnalysisTool  
PaperAnalysisTool.__init__(llm)            # Initialize with LLM
PaperAnalysisTool.analyze_paper() -> str   # Analyze paper relevance
```

#### Workflow Steps:
```
1. refine_query    â†’ Optimize user query for academic search
2. search_papers   â†’ Find papers via Google Scholar API
3. process_documents â†’ Download & extract paper content
4. extract_context â†’ Find relevant text chunks
5. generate_answer â†’ Create comprehensive response
6. error_handler   â†’ Handle any workflow failures
```

#### Dependencies & Calls:
```
â†’ GoogleScholarAPI (from scholar_api.py)
â†’ DocumentProcessor (from document_processor.py)  
â†’ VectorStore (from vector_store.py)
â†’ GeminiClient (from gemini_client.py)
â†’ config (from config.py)
â†’ LangGraph, LangChain Core
```

---

### 4. **ğŸ“„ document_processor.py** - Document Processing
**PDF & HTML Content Extraction**

#### Classes:
- `DocumentChunk` - Data container for text chunks
- `DocumentProcessor` - Main processing engine

#### Key Methods:
```python
# DocumentProcessor
DocumentProcessor.__init__(cache_dir: str)              # Initialize with cache
DocumentProcessor.process_paper(url, title) -> List     # Main processing entry
DocumentProcessor._download_document(url) -> bytes      # Download from URL
DocumentProcessor._process_pdf(content, title) -> List  # Extract from PDF
DocumentProcessor._process_html(content, title) -> List # Extract from HTML  
DocumentProcessor._extract_sections(soup) -> Dict       # Parse HTML sections
DocumentProcessor._clean_text(text: str) -> str         # Clean extracted text
DocumentProcessor._chunk_text(text, max_size) -> List   # Split into chunks
DocumentProcessor._generate_cache_key(url) -> str       # Create cache key
DocumentProcessor._get_from_cache(key) -> List          # Retrieve from cache
DocumentProcessor._save_to_cache(key, chunks)           # Save to cache

# DocumentChunk  
DocumentChunk.text           # The actual text content
DocumentChunk.metadata       # Title, page, section info
DocumentChunk.source         # Original document URL
DocumentChunk.page_number    # Page number (if PDF)
DocumentChunk.section        # Section name (if HTML)
```

#### Processing Flow:
```
URL â†’ Download â†’ [PDF/HTML Detection] â†’ Extract Text â†’ Clean â†’ Chunk â†’ Cache â†’ Return
```

#### Dependencies & Calls:
```
â†’ PyPDF2, BeautifulSoup4, aiohttp
â†’ Called by: ResearchAgent._process_documents()
â†’ Outputs to: VectorStore.add_documents()
```

---

### 5. **ğŸ—ƒï¸ vector_store.py** - Vector Database Management
**ChromaDB Integration & Semantic Search**

#### Classes:
- `VectorStore` - Main vector database interface

#### Key Methods:
```python
VectorStore.__init__(persist_dir, embedding_model)     # Initialize ChromaDB
VectorStore.add_documents(chunks) -> bool              # Store document chunks
VectorStore.similarity_search(query, k) -> List       # Semantic search
VectorStore.search_by_paper(title, query) -> List     # Search within paper
VectorStore.get_paper_content(title) -> List          # Get all paper content
VectorStore.delete_paper(title) -> bool               # Remove paper data
VectorStore.get_collection_stats() -> Dict            # Database statistics
VectorStore.clear_collection() -> bool                # Clear all data
VectorStore.hybrid_search(query, k) -> List           # Combined search
VectorStore._keyword_search(query, k) -> List         # Keyword matching
VectorStore._combine_search_results() -> List         # Merge search results
```

#### Search Flow:
```
Query â†’ [Semantic + Keyword Search] â†’ Rank Results â†’ Return Top K
```

#### Dependencies & Calls:
```
â†’ ChromaDB, SentenceTransformers
â†’ Called by: ResearchAgent (for context extraction)
â†’ Input from: DocumentProcessor (document chunks)
```

---

### 6. **ğŸ” scholar_api.py** - Google Scholar Integration
**Academic Paper Discovery**

#### Classes:
- `ScholarResult` - Paper metadata container
- `GoogleScholarAPI` - Scholar search interface

#### Key Methods:
```python
# GoogleScholarAPI
GoogleScholarAPI.__init__(api_key)                    # Initialize with SerpAPI
GoogleScholarAPI.search_papers(query, max) -> List   # Search for papers
GoogleScholarAPI._parse_serpapi_results() -> List    # Parse API response
GoogleScholarAPI.refine_query(query) -> str          # Add academic keywords

# ScholarResult
ScholarResult.title          # Paper title
ScholarResult.authors        # Author list
ScholarResult.abstract       # Paper abstract
ScholarResult.year           # Publication year
ScholarResult.citation_count # Citation count
ScholarResult.pdf_url        # PDF download link
ScholarResult.scholar_url    # Google Scholar page
ScholarResult.venue          # Publication venue
```

#### Search Flow:
```
Query â†’ SerpAPI â†’ Google Scholar â†’ Parse Results â†’ Return ScholarResult[]
```

#### Dependencies & Calls:
```
â†’ SerpAPI (Google Search API)
â†’ Called by: ResearchAgent._search_papers()
```

---

### 7. **ğŸ§  gemini_client.py** - Google Gemini AI Client
**AI Language Model Interface**

#### Classes:
- `GeminiClient` - Main Gemini API wrapper

#### Key Methods:
```python
GeminiClient.__init__(api_key, model, temp, tokens)   # Initialize Gemini
GeminiClient.test_connection() -> bool                # Test API access
GeminiClient.get_model_info() -> Dict                 # Model information
GeminiClient.invoke(messages) -> AIMessage           # Generate response
GeminiClient._convert_messages_to_text() -> str      # Format input
GeminiClient.ainvoke(messages) -> AIMessage          # Async version
```

#### AI Flow:
```
Messages â†’ Format â†’ Gemini API â†’ Safety Check â†’ Parse Response â†’ AIMessage
```

#### Dependencies & Calls:
```
â†’ google.generativeai
â†’ LangChain Core (for message compatibility)
â†’ Called by: ResearchAgent (all AI operations)
```

---

### 8. **ğŸ”§ geminiClientWrapper.py** - Alternative Client
**Alternative Gemini Implementation**

#### Classes:
- `ChatMessage` - Simple message format
- `GeminiClient` - Alternative implementation
- `ChatPromptTemplate` - Prompt formatting

#### Key Methods:
```python
GeminiClient.invoke(messages) -> ChatMessage          # Sync generation
GeminiClient.ainvoke(messages) -> ChatMessage         # Async generation
GeminiClient.chat(message) -> str                     # Simple chat
GeminiClient.achat(message) -> str                    # Async chat
GeminiClient.get_model_info() -> Dict                 # Model details
GeminiClient.test_connection() -> bool                # Connection test
ChatPromptTemplate.from_template(template)           # Create template
ChatPromptTemplate.format_messages(**kwargs)         # Format with data
```

---

### 9. **ğŸ”§ health_check.py & logging_config.py.py** - Logging Setup
**Application Logging Configuration**

#### Functions:
```python
setup_logging(log_level, log_file) -> Logger         # Configure logging
```

#### Logging Setup:
```
Console Handler â†’ File Handler (Rotating) â†’ Logger Configuration
```

---

### 10. **ğŸ§ª Test Files** - Quality Assurance

#### **test_gemini.py** - Gemini API Tests
```python
test_gemini_connection()           # Test API connectivity
test_simple_generation()           # Test basic text generation
test_research_query_processing()   # Test query refinement
main()                            # Run all tests
```

#### **test_full_integration.py** - End-to-End Tests
```python
test_research_agent()             # Full workflow test
test_gemini_model_comparison()    # Model comparison
main()                           # Integration test runner
```

---

## ğŸ”„ **Complete Execution Flow**

### **ğŸš€ Application Startup**
```
1. main() in app.py
2. StreamlitApp.__init__()
3. StreamlitApp.initialize_agent()
4. ResearchAgent.__init__()
   â”œâ”€â”€ GeminiClient.__init__()
   â”œâ”€â”€ GoogleScholarAPI.__init__()
   â”œâ”€â”€ DocumentProcessor.__init__()
   â”œâ”€â”€ VectorStore.__init__()
   â””â”€â”€ _build_graph() â†’ LangGraph workflow
```

### **ğŸ” Query Processing Workflow** 
```
User Query â†’ app.py
    â†“
StreamlitApp.process_query()
    â†“
ResearchAgent.research() â†’ LangGraph Execution:
    â†“
1. _refine_query()
   â””â”€â”€ QueryRefinementTool.refine_query()
       â””â”€â”€ GeminiClient.invoke()
    â†“
2. _search_papers()
   â””â”€â”€ GoogleScholarAPI.search_papers()
       â””â”€â”€ SerpAPI â†’ Google Scholar
    â†“
3. _process_documents()
   â””â”€â”€ DocumentProcessor.process_paper()
       â”œâ”€â”€ _download_document()
       â”œâ”€â”€ _process_pdf() OR _process_html()
       â”œâ”€â”€ _chunk_text()
       â””â”€â”€ VectorStore.add_documents()
    â†“
4. _extract_context()
   â””â”€â”€ VectorStore.hybrid_search()
       â”œâ”€â”€ similarity_search() (semantic)
       â””â”€â”€ _keyword_search() (keyword)
    â†“
5. _generate_answer()
   â””â”€â”€ GeminiClient.invoke() â†’ Final Response
    â†“
Return to app.py â†’ Display Results
```

---

## ğŸ¯ **Key Components Interaction Map**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Streamlit UI  â”‚â”€â”€â”€â”€â”‚  Research Agent  â”‚â”€â”€â”€â”€â”‚   Gemini AI     â”‚
â”‚   (app.py)      â”‚    â”‚ (research_agent) â”‚    â”‚ (gemini_client) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                        â”‚                        â”‚
         â”‚                        â–¼                        â”‚
         â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
         â”‚              â”‚  Google Scholar  â”‚               â”‚
         â”‚              â”‚  (scholar_api)   â”‚               â”‚
         â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
         â”‚                        â”‚                        â”‚
         â–¼                        â–¼                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Config        â”‚    â”‚ Document Processorâ”‚    â”‚  Vector Store   â”‚
â”‚   (config.py)   â”‚    â”‚(document_processor)â”‚   â”‚ (vector_store)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚                        â”‚
                               â–¼                        â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
                    â”‚    Cache         â”‚               â”‚
                    â”‚  (file system)   â”‚               â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
                                                      â”‚
                                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                         â”‚   ChromaDB      â”‚
                                         â”‚   (database)    â”‚
                                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š **Technology Stack**

### **ğŸ¨ Frontend**
- **Streamlit** - Web interface
- **Plotly** - Data visualizations  
- **Pandas** - Data manipulation

### **ğŸ¤– AI & ML**
- **Google Gemini 2.5 Flash** - Language model
- **SentenceTransformers** - Text embeddings
- **LangGraph** - Workflow orchestration
- **LangChain Core** - AI framework components

### **ğŸ” Search & Data**
- **SerpAPI** - Google Scholar access
- **ChromaDB** - Vector database
- **PyPDF2** - PDF processing
- **BeautifulSoup4** - HTML parsing
- **aiohttp** - Async HTTP requests

### **ğŸ”§ Infrastructure**
- **Python 3.11+** - Runtime
- **asyncio** - Async programming
- **dotenv** - Environment management
- **logging** - Application logging

---

## ğŸ¯ **Summary**

This Research Agent is a **sophisticated AI-powered academic research assistant** that:

1. **ğŸ¨ Provides** an intuitive Streamlit web interface
2. **ğŸ” Searches** Google Scholar for relevant academic papers
3. **ğŸ“„ Processes** PDF and HTML documents automatically  
4. **ğŸ§  Uses** Google Gemini AI for intelligent query processing
5. **ğŸ—ƒï¸ Stores** document embeddings in a vector database
6. **ğŸ¤– Generates** comprehensive, citation-backed responses
7. **ğŸ“Š Visualizes** research insights with interactive charts

The architecture follows **modern best practices** with clear separation of concerns, async processing, comprehensive error handling, and extensive testing coverage.
